import pandas as pd
from collections import Counter
from tqdm import tqdm
import chardet
import MeCab
import os
import pickle
import gzip
import psutil
import cupy as cp
import multiprocessing
import itertools

# -----------------------
# ì „ì—­ ì„¤ì •: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì €ì¥ ë””ë ‰í„°ë¦¬
# -----------------------
CHECKPOINT_DIR = "checkpoints"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)  # checkpoints í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±

# -----------------------
# íŒŒì¼ ë¡œë”© í•¨ìˆ˜
# -----------------------
def read_csv_with_encoding(csv_file):
    with open(csv_file, 'rb') as f:
        result = chardet.detect(f.read(10000))
    return pd.read_csv(csv_file, encoding=result['encoding'])

# -----------------------
# ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ì „ì—­ íƒœê±° ì¬ì‚¬ìš©)
# -----------------------
# ë©”ì¸ í”„ë¡œì„¸ìŠ¤ìš© íƒœê±°
tagger = MeCab.Tagger()

def extract_nouns(text, min_len=2):
    parsed = tagger.parse(text)
    nouns = set()
    for line in parsed.splitlines():
        if line == 'EOS':
            break
        try:
            word, features = line.split('\t')
            pos = features.split(',')[0]
            if pos in ['NNG', 'NNP'] and len(word) >= min_len:
                nouns.add(word)
        except ValueError:
            continue
    return sorted(nouns)

# -----------------------
# ëª…ì‚¬ ì‚¬ì „ ìºì‹œ í™œìš©
# -----------------------
def build_or_load_noun_dictionary(texts):
    cache_path = "noun_mapping.pkl"
    if os.path.exists(cache_path):
        print(f"\nğŸ“¦ ëª…ì‚¬ ì‚¬ì „ ìºì‹œ ë°œê²¬ â†’ {cache_path} ë¡œë“œ ì¤‘...")
        with open(cache_path, "rb") as f:
            data = pickle.load(f)
        return data['noun2id'], data['id2noun']

    print("\nğŸ†• ëª…ì‚¬ ì‚¬ì „ ìºì‹œ ì—†ìŒ â†’ ìƒˆë¡œ ìƒì„± ì¤‘...")
    noun_set = set()
    for text in tqdm(texts, desc="ğŸ” ëª…ì‚¬ ì‚¬ì „ ìƒì„± ì¤‘"):
        noun_set.update(extract_nouns(text))
    noun2id = {noun: i for i, noun in enumerate(sorted(noun_set))}
    id2noun = {i: noun for noun, i in noun2id.items()}

    with open(cache_path, "wb") as f:
        pickle.dump({'noun2id': noun2id, 'id2noun': id2noun}, f)
    print("âœ… ëª…ì‚¬ ì‚¬ì „ ì €ì¥ ì™„ë£Œ")
    return noun2id, id2noun

# -----------------------
# ëª…ì‚¬ ìˆ˜ ìºì‹œ í™œìš© (ê° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ëª…ì‚¬ì˜ ê°œìˆ˜)
# -----------------------
def build_or_load_noun_count_cache(texts, cache_path):
    if os.path.exists(cache_path):
        print(f"\nğŸ“¦ ëª…ì‚¬ ìˆ˜ ìºì‹œ ë°œê²¬ â†’ {cache_path} ë¡œë“œ ì¤‘...")
        with open(cache_path, "rb") as f:
            noun_counts = pickle.load(f)
        return noun_counts
    print(f"\nğŸ†• ëª…ì‚¬ ìˆ˜ ìºì‹œ ì—†ìŒ â†’ ìƒˆë¡œ ìƒì„± ì¤‘... ({cache_path})")
    noun_counts = []
    for text in tqdm(texts, desc="ğŸ” ëª…ì‚¬ ìˆ˜ ìºì‹œ ìƒì„± ì¤‘"):
        nouns = extract_nouns(text)
        noun_counts.append(len(nouns))
    with open(cache_path, "wb") as f:
        pickle.dump(noun_counts, f)
    print("âœ… ëª…ì‚¬ ìˆ˜ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    return noun_counts

# -----------------------
# ëª…ì‚¬ 3ê°œ ì¡°í•© ì¶”ì¶œ í•¨ìˆ˜ (GPU ì‚¬ìš©, ë‹¨ ê³¼ë„í•œ ëª…ì‚¬ ìˆ˜ì¼ ê²½ìš° CPU ë°©ì‹ìœ¼ë¡œ ì „í™˜)
# -----------------------
def extract_triplets(text, noun2id, cpu_threshold=50):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•œ í›„, ëª…ì‚¬ IDë¥¼ ì´ìš©í•´ 3ê°œ ì¡°í•©(triplet)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ëª…ì‚¬ì˜ ê°œìˆ˜ê°€ cpu_thresholdë³´ë‹¤ í¬ë©´ CPU ê¸°ë°˜ ì¡°í•© ìƒì„±ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
    """
    if not isinstance(text, str):
        return []
    nouns = extract_nouns(text)
    ids = [noun2id[n] for n in nouns if n in noun2id]
    ids = sorted(ids)
    if len(ids) < 3:
        return []
    # ëª…ì‚¬ ìˆ˜ê°€ ë§ìœ¼ë©´ CPU ë°©ì‹ (itertools)ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ë¶€ë‹´ ì™„í™”
    if len(ids) > cpu_threshold:
        return list(itertools.combinations(ids, 3))
    # GPU ë°©ì‹: cupyë¥¼ ì´ìš©í•œ ë©”ì‰¬ê·¸ë¦¬ë“œ ìƒì„± (ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì ë‹¹í•œ ê²½ìš°)
    cp_ids = cp.array(ids)
    n = cp_ids.shape[0]
    indices = cp.arange(n)
    i, j, k = cp.meshgrid(indices, indices, indices, indexing='ij')
    mask = (i < j) & (j < k)
    triplet_indices = cp.stack([i[mask], j[mask], k[mask]], axis=-1)
    comb_gpu = cp_ids[triplet_indices]
    comb_list = [tuple(map(int, row)) for row in cp.asnumpy(comb_gpu)]
    return comb_list

# -----------------------
# ë™ì  ë°°ì¹˜ êµ¬ì„± í•¨ìˆ˜
# -----------------------
def dynamic_batching(texts, noun_counts, max_combinations=1_000_000):
    """
    ê° í…ìŠ¤íŠ¸ì˜ ëª…ì‚¬ ìˆ˜ì— ë”°ë¼ ì˜ˆìƒ ì¡°í•© ìˆ˜(nC3)ë¥¼ ê³„ì‚°í•œ í›„,
    ëˆ„ì  ì˜ˆìƒ ì¡°í•© ìˆ˜ê°€ max_combinationsë¥¼ ë„˜ì§€ ì•ŠëŠ” ë²”ìœ„ ë‚´ì—ì„œ ë°°ì¹˜ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    batches = []
    current_batch = []
    current_total = 0
    for text, count in zip(texts, noun_counts):
        if count < 3:
            estimated_combs = 0
        else:
            estimated_combs = (count * (count - 1) * (count - 2)) // 6
        # ë§Œì•½ í•œ í…ìŠ¤íŠ¸ê°€ ë‹¨ë…ìœ¼ë¡œ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ë³„ë„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        if estimated_combs > max_combinations:
            if current_batch:
                batches.append(current_batch)
                current_batch = []
                current_total = 0
            batches.append([text])
            continue
        # í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€ ì‹œ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ í˜„ì¬ ë°°ì¹˜ë¥¼ ë¶„ë¦¬
        if current_total + estimated_combs > max_combinations and current_batch:
            batches.append(current_batch)
            current_batch = []
            current_total = 0
        current_batch.append(text)
        current_total += estimated_combs
    if current_batch:
        batches.append(current_batch)
    return batches

# -----------------------
# Counterë¥¼ ë””ìŠ¤í¬ì— ë¶„ì‚° ì €ì¥
# -----------------------
def save_partial_counter(counter, prefix, batch_id):
    fname = os.path.join(CHECKPOINT_DIR, f"{prefix}_partial_{batch_id}.pkl.gz")
    with gzip.open(fname, "wb") as f:
        pickle.dump(counter, f)

# -----------------------
# ë¶€ë¶„ ì²´í¬í¬ì¸íŠ¸ ë³‘í•© í•¨ìˆ˜
# -----------------------
def merge_partial_checkpoints(prefix, file_pattern_prefix, merge_threshold=100):
    """
    CHECKPOINT_DIR ë‚´ì— file_pattern_prefixë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì´ merge_threshold ì´ìƒ ìŒ“ì´ë©´,
    í•´ë‹¹ íŒŒì¼ë“¤ì„ ë³‘í•©í•œ í›„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ì›ë³¸ íŒŒì¼ë“¤ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    files = sorted([f for f in os.listdir(CHECKPOINT_DIR) 
                    if f.startswith(file_pattern_prefix) and f.endswith('.pkl.gz')])
    if len(files) >= merge_threshold:
        print(f"Merging {len(files)} checkpoint files for {prefix}...")
        total_counter = Counter()
        for fname in files:
            full_path = os.path.join(CHECKPOINT_DIR, fname)
            with gzip.open(full_path, 'rb') as f:
                part = pickle.load(f)
                total_counter.update(part)
            os.remove(full_path)
        merged_filename = os.path.join(CHECKPOINT_DIR, f"{prefix}_merged.pkl.gz")
        with gzip.open(merged_filename, "wb") as f:
            pickle.dump(total_counter, f)
        print(f"Merged file saved as {merged_filename}")
        return total_counter
    return None

# -----------------------
# ë¶„ì‚°ëœ Counter ë³‘í•© í•¨ìˆ˜ (ìµœì¢… ê²°ê³¼)
# -----------------------
def merge_all_counters(prefix):
    total = Counter()
    # partial íŒŒì¼ê³¼ ë³‘í•©ëœ íŒŒì¼ ëª¨ë‘ í¬í•¨
    files = [f for f in os.listdir(CHECKPOINT_DIR) 
             if (f.startswith(f"{prefix}_partial_") or f.startswith(f"{prefix}_merged_")) and f.endswith('.pkl.gz')]
    for fname in sorted(files):
        full_path = os.path.join(CHECKPOINT_DIR, fname)
        with gzip.open(full_path, 'rb') as f:
            part = pickle.load(f)
            total.update(part)
        os.remove(full_path)
    return total

# -----------------------
# ì›Œì»¤ ì´ˆê¸°í™” í•¨ìˆ˜ (multiprocessing)
# -----------------------
def init_worker(noun2id_arg):
    global noun2id_global, tagger
    noun2id_global = noun2id_arg
    # ê° ì›Œì»¤ì—ì„œ ë³„ë„ì˜ íƒœê±° ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
    tagger = MeCab.Tagger()

def process_text_wrapper(text):
    return extract_triplets(text, noun2id_global)

# -----------------------
# ë™ì  ë°°ì¹˜ ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸ ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------
def extract_with_checkpoint_parallel_dynamic(batches, filename_prefix, noun2id, num_workers=4, merge_threshold=100):
    tmp_file = os.path.join(CHECKPOINT_DIR, f"{filename_prefix}_progress.pkl")
    start_batch_idx = 0
    if os.path.exists(tmp_file):
        with open(tmp_file, "rb") as f:
            saved = pickle.load(f)
            start_batch_idx = saved.get("last_batch", -1) + 1
        print(f"\nğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œë¨: ë°°ì¹˜ {start_batch_idx}ë¶€í„° ì‹œì‘")
    total_batches = len(batches)
    pbar = tqdm(total=total_batches, initial=start_batch_idx, desc=filename_prefix)
    try:
        with multiprocessing.Pool(processes=num_workers, initializer=init_worker, initargs=(noun2id,)) as pool:
            for batch_idx in range(start_batch_idx, total_batches):
                batch_texts = batches[batch_idx]
                results = pool.imap_unordered(process_text_wrapper, batch_texts)
                batch_counter = Counter()
                for triplets in tqdm(results, total=len(batch_texts), desc=f"Batch {batch_idx} ì§„í–‰", leave=False):
                    batch_counter.update(triplets)
                save_partial_counter(batch_counter, filename_prefix, batch_idx)
                with open(tmp_file, "wb") as f:
                    pickle.dump({"last_batch": batch_idx}, f)
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ë¶€ë¶„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë³‘í•©
                merge_partial_checkpoints(filename_prefix, f"{filename_prefix}_partial_", merge_threshold)
                
                pbar.update(1)
    finally:
        pbar.close()
        # ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ ì—¬ë¶€: í•„ìš”ì— ë”°ë¼ ì•„ë˜ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ í•´ì œ
        # if os.path.exists(tmp_file):
        #     os.remove(tmp_file)
    return merge_all_counters(filename_prefix)

# -----------------------
# ë©”ì¸ ë¡œì§
# -----------------------
def analyze_keyword_triplets(csv_path):
    df = read_csv_with_encoding(csv_path)

    # 'body' ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½ ìˆìŒ/ì—†ìŒ ë¶„ë¦¬
    with_summary = df[df['summary'].notnull() & (df['summary'].str.strip() != '')]['body'].dropna().tolist()
    without_summary = df[df['summary'].isnull() | (df['summary'].str.strip() == '')]['body'].dropna().tolist()

    print(f"ìš”ì•½ ìˆìŒ ê¸°ì‚¬ ìˆ˜: {len(with_summary)}")
    print(f"ìš”ì•½ ì—†ìŒ ê¸°ì‚¬ ìˆ˜: {len(without_summary)}")

    all_texts = with_summary + without_summary
    noun2id, id2noun = build_or_load_noun_dictionary(all_texts)

    # ëª…ì‚¬ ìˆ˜ ìºì‹œ ìƒì„± (with_summaryì™€ without_summary ê°ê°)
    with_count_cache_path = "noun_count_cache_with.pkl"
    without_count_cache_path = "noun_count_cache_without.pkl"
    with_noun_counts = build_or_load_noun_count_cache(with_summary, with_count_cache_path)
    without_noun_counts = build_or_load_noun_count_cache(without_summary, without_count_cache_path)

    # ë™ì  ë°°ì¹˜ êµ¬ì„± (max_combinations ê°’ì€ í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ë§ê²Œ ì¡°ì ˆ)
    max_combinations = 1_000_000
    with_batches = dynamic_batching(with_summary, with_noun_counts, max_combinations)
    without_batches = dynamic_batching(without_summary, without_noun_counts, max_combinations)

    print(f"\nwith_summary: ë™ì  ë°°ì¹˜ ê°œìˆ˜ = {len(with_batches)}")
    print(f"without_summary: ë™ì  ë°°ì¹˜ ê°œìˆ˜ = {len(without_batches)}")

    # ë™ì  ë°°ì¹˜ë³„ë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with_counter = extract_with_checkpoint_parallel_dynamic(with_batches, "with_summary", noun2id, num_workers=4, merge_threshold=100)
    without_counter = extract_with_checkpoint_parallel_dynamic(without_batches, "without_summary", noun2id, num_workers=4, merge_threshold=100)

    # ìš”ì•½ ì—†ìŒ ë‰´ìŠ¤ì—ë§Œ ë“±ì¥í•˜ëŠ” 3ê°œ ì¡°í•© í†µê³„ ì‚°ì¶œ
    full_triplet_stats = []
    for triplet, without_count in without_counter.items():
        with_count = with_counter.get(triplet, 0)
        if with_count == 0 and without_count >= 3:
            keywords = [id2noun[i] for i in triplet]
            full_triplet_stats.append({
                "keyword1": keywords[0],
                "keyword2": keywords[1],
                "keyword3": keywords[2],
                "without_summary_count": without_count,
                "with_summary_count": with_count,
                "frequency_gap": without_count - with_count
            })

    full_triplet_stats = sorted(full_triplet_stats, key=lambda x: x["without_summary_count"], reverse=True)

    print("\nğŸ§© ìš”ì•½ ì—†ìŒ ë‰´ìŠ¤ì—ë§Œ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ 3ê°œ ì¡°í•© Top 30:")
    for item in full_triplet_stats[:30]:
        print(f"{item['keyword1']}, {item['keyword2']}, {item['keyword3']}: {item['without_summary_count']}")

    df_result = pd.DataFrame(full_triplet_stats)
    result_csv = "keyword_triplet_stats.csv"
    df_result.to_csv(result_csv, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ë¨ â†’ {result_csv}")

# -----------------------
# ì¶”ê°€: CSV í›„ì²˜ë¦¬ (í•„í„°ë§) í•¨ìˆ˜
# -----------------------
def filter_triplets_csv(input_csv_path, output_csv_path):
    import pandas as pd
    # tqdmì˜ notebook ë²„ì „ì„ ì‚¬ìš© (ë…¸íŠ¸ë¶ í™˜ê²½ì´ë¼ë©´)
    from tqdm.notebook import tqdm

    # CSV ì½ê¸°
    df = pd.read_csv(input_csv_path)

    # ì œê±°í•  í‚¤ì›Œë“œ ëª©ë¡ (ì™„ì „ ì¼ì¹˜)
    exact_keywords_to_remove = ['ì œë³´', 'ì¹´ì¹´ì˜¤í†¡', 'ì—°í•©ë‰´ìŠ¤', 'ë‰´ìŠ¤','ì €ì‘ê¶Œì', 'ì†¡ê³ ', 'ë°°í¬', 'ê¸ˆì§€', 'ê¸°ì', 'í•™ìŠµ','ë¬´ë‹¨', 'ì‚¼ì„±ì „ì', 'ì´íˆ¬ë°ì´','í•œê²¨ë¡€',
                            'ë°˜ë„ì²´','ë©”ëª¨ë¦¬','ì œì¡°','ê³µê¸‰','íˆ¬ì','ì‚°ì—…','ì „í™˜','í™•ëŒ€','ê³ ê°','ì‹¤ì ','ì „ë§','í™•ì¸','íŠ¸ëŸ¼í”„','í–‰ì •ë¶€','ìš°ë ¤','ì§€ì†','ì œê³µ',
                            'ëŠ¥ë ¥','ì„¸ëŒ€','ì„¼í„°','ë°ì´í„°','ì²¨ë‹¨', 'ê²©ì°¨', 'ì„±ì¥', 'ê²½ì˜', 'ê³µê¸‰', 'ê¸°ì—…', 'ë¶€ì¡±','ê·¸ë£¹','ì‚¬ì—…', 'ê°œë°œ','ë°ì¼ë¦¬','ê³µì •','ì œì¡°',
                            'ì„¤ë¹„', 'ê¸€ë¡œë²Œ','ì‡¼í¬', 'ê²½ì œ', 'í•œêµ­','ê³¼í•™','ì•„ì‹œì•„','ë¯¸ë˜','ì£¼ë„', 'ìš´ì˜','ê°€ëŠ¥','í•œê²¨ë ˆ','ê°ì†Œ','ì‹œì¥','í˜ì‹ ','ì¤‘êµ­','ê¸°íšŒ','ë¯¸êµ­',
                            'ê²½ìŸë ¥','AI','ì¸ê³µì§€ëŠ¥','ì¸ë„','ìˆ˜ì¶œ','ìˆ˜ì…','ì˜ì—…','ê·¹ë³µ','ê°•í™”','íŒŒìš´ë“œë¦¬']
    # ì œê±°í•  í‚¤ì›Œë“œ ëª©ë¡ (ë¶€ë¶„ í¬í•¨)
    partial_keywords_to_remove = ['ì¼ë³´', 'ì‹ ë¬¸','ì‚¼ì„±','í•œêµ­','ì „ì']

    tqdm.pandas(desc="Filtering rows (fast)")

    # 1. ì™„ì „ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ì œê±°: ê° ì…€ì˜ ê°’ì´ exact_keywords_to_removeì— ì™„ì „íˆ ì¼ì¹˜í•˜ë©´ í•´ë‹¹ í–‰ ì œê±°
    mask_exact = df[['keyword1', 'keyword2', 'keyword3']].progress_apply(
        lambda row: not any(word in exact_keywords_to_remove for word in row),
        axis=1
    )
    df_exact_filtered = df[mask_exact].reset_index(drop=True)

    # 2. ì¼ë¶€ë¼ë„ í¬í•¨ë˜ë©´ ì œê±°: ê° ì…€ì˜ ê°’ ë‚´ì— partial_keywords_to_removeì— í¬í•¨ëœ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ í•´ë‹¹ í–‰ ì œê±°
    mask_partial = df_exact_filtered[['keyword1', 'keyword2', 'keyword3']].progress_apply(
        lambda row: not any(kw in word for word in row for kw in partial_keywords_to_remove),
        axis=1
    )
    filtered_df = df_exact_filtered[mask_partial].reset_index(drop=True)

    print("\nğŸ§¹ í•„í„°ë§ ì™„ë£Œëœ ê²°ê³¼:")
    print(filtered_df)
    filtered_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“ ìµœì¢… ê²°ê³¼ ì €ì¥ë¨ â†’ {output_csv_path}")

# -----------------------
# ë©”ì¸ ì§„ì…ì 
# -----------------------
if __name__ == '__main__':
    # í”Œë«í¼ì— ë”°ë¼ ì ì ˆí•œ start method ì„ íƒ (UnixëŠ” 'spawn' ì‚¬ìš© ê¶Œì¥)
    if os.name == 'posix':
        multiprocessing.set_start_method('spawn', force=True)
    else:
        multiprocessing.set_start_method('spawn', force=True)
    
    # ì›ë³¸ CSV íŒŒì¼ ê²½ë¡œ (ì˜ˆì‹œ)
    csv_path = "/home/kororu/KoBERT/articles_summary24.csv"
    analyze_keyword_triplets(csv_path)
    
    # ë¶„ì„ í›„ ìƒì„±ëœ keyword_triplet_stats.csv íŒŒì¼ì„ í›„ì²˜ë¦¬í•˜ì—¬ í•„í„°ë§ ì‹¤í–‰
    filter_triplets_csv("keyword_triplet_stats.csv", "3keywords.csv")
