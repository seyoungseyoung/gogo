import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm

# ---------------------------
# ì„¤ì •
# ---------------------------
article_folder_path = "/home/kororu/KoBERT"
tfidf_top_percentile = 0.2  # ìƒìœ„ 20%ëŠ” 'ì‚´ë ¤ì•¼í•¨'ìœ¼ë¡œ ë¶„ë¥˜

# ---------------------------
# í‚¤ì›Œë“œ ì •ì˜
# ---------------------------
must_keep_keywords = [
    'ì˜ì—…ì´ìµ', 'ì–´ë‹', 'ìˆœì´ìµ', 'ë§¤ì¶œ', 'ê°ìµ', 'ì ì', 'ì–´ë‹ ì‡¼í¬',
    'ê¸‰ê°', 'ê¸‰ë“±', 'í•˜ë½ì„¸', 'ìƒìŠ¹ì„¸', 'ê°ì‚°', 'ìˆ˜ì¶œ', 'ì¶œí•˜ëŸ‰',
    'ê³µê¸‰ ë¶€ì¡±', 'ê³µê¸‰ ê³¼ì‰', 'í™˜ìœ¨', 'íˆ¬ìê³„íš', 'ë³€ë™ì„±', 'ë§¤ë„ì„¸', 'ë§¤ìˆ˜ì„¸',
    'ê²½ì˜ê¶Œ', 'í•©ë³‘', 'íŒŒì—…', 'ì¦ì'
]


irrelevant_keywords = [
    'í•™ìƒ', 'ì‚¬íšŒê³µí—Œ', 'ë‚˜ëˆ”', 'ë´‰ì‚¬í™œë™', 'ì¥í•™ê¸ˆ',
    'ì‚¬ë‚´ í–‰ì‚¬', 'ìˆ˜ìƒ', 'ì‹œìƒì‹', 'ì¶•ì‚¬', 'ê¸°ë…ì‹', 'ë´‰ì‚¬', 'ì‚¬ë³´', 'í–‰ì‚¬ ê°œìµœ',
    'ìº í˜ì¸', 'ê¸°ë…í–‰ì‚¬', 'ì›Œí¬ìˆ', 'ì‚¬ë³´ë°œê°„', 'ì¸í„°ë·°', 'ê¸°ê³ ë¬¸',
    'í›„ì›', 'ì‚¬íšŒì ê¸°ì—…', 'ë©˜í† ë§', 'êµìœ¡ê¸°ë¶€', 'ë¬¸í™”í–‰ì‚¬',
    'ìì›ë´‰ì‚¬', 'ì¶•ì œ', 'ì†Œì…œë²¤ì²˜', 'ì‹œë¯¼ì°¸ì—¬', 'ì§€ì—­ì‚¬íšŒ',
    'ì „ì‹œíšŒ', 'ì‚¬ì§„ì „', 'ì‚¬ë‚´ì†Œì‹', 'ê³µë¡œìƒ', 'ì‚¬íšŒì ì±…ì„', 'ë‚˜ëˆ”í™œë™',
    'ê³µí—Œí™œë™', 'ê¸°ì—…ë¬¸í™”', 'ì²´í—˜í™œë™', 'í–‰ì‚¬ì°¸ì—¬', 'ì´ë²¤íŠ¸',
    'ë¬¸í™”ê³µì—°', 'í™ë³´ëŒ€ì‚¬', 'ì‚¬ë‚´ë™í˜¸íšŒ', 'í™˜ê²½ì •í™”', 'ì§€ì—­í–‰ì‚¬',
    'ìŠ¤í°ì„œ', 'ì°¸ì—¬í–‰ì‚¬'
]

training_keywords = [
    'ì•„ì¹´ë°ë¯¸', 'ì¸ì¬ì–‘ì„±', 'ìˆ˜ë£Œìƒ', 'ì–‘ì„±ê³¼ì •', 'êµìœ¡ìƒ', 'í•©ìˆ™',
    'ê°œë°œì', 'êµìœ¡ í”„ë¡œê·¸ë¨', 'ì»¤ë¦¬í˜ëŸ¼', 'ì·¨ì—… ì—°ê³„', 'SSA', 'SSAFY', 'AIVLE', 'ì—ì´ë¸” ìŠ¤ì¿¨',
    'ë¶€íŠ¸ìº í”„', 'êµìœ¡ì„¼í„°', 'ì˜¨ë³´ë”© êµìœ¡', 'ì‹ ì…êµìœ¡', 'ê¸°ìˆ êµìœ¡',
    'ì§ë¬´êµìœ¡', 'ì‚¬ë‚´êµìœ¡', 'ë¦¬ë”ì‹­ êµìœ¡', 'ì—°ìˆ˜ê³¼ì •', 'ì§ì› ì—°ìˆ˜',
    'ì§ë¬´ì—­ëŸ‰ ê°•í™”', 'ê¸°ì´ˆêµìœ¡', 'í˜„ì¥ì‹¤ìŠµ', 'ì·¨ì—…ìº í”„', 'ì—­ëŸ‰ê°œë°œ',
    'ë©˜í† ë§ í”„ë¡œê·¸ë¨', 'ì˜¨ë¼ì¸ ê°•ì˜', 'MOOC', 'í•™ìŠµê³¼ì •', 'ì§‘ì²´êµìœ¡',
    'êµìœ¡ê³¼ì • ê°œì„¤', 'í…Œí¬ ìº í”„', 'ì½”ë”© êµìœ¡', 'AI ì•„ì¹´ë°ë¯¸',
    'ì‚¬ë‚´ê°•ì‚¬', 'êµìœ¡ì´ìˆ˜', 'ìê²©ì¦ ê³¼ì •', 'HRD í”„ë¡œê·¸ë¨'
]

# ---------------------------
# í•¨ìˆ˜ ì •ì˜
# ---------------------------
def is_must_keep(text):
    if not isinstance(text, str):
        return False
    return any(keyword in text for keyword in must_keep_keywords)

def is_irrelevant_context(text):
    if not isinstance(text, str):
        return False
    return any(keyword in text for keyword in irrelevant_keywords)

def is_training_news(text):
    if not isinstance(text, str):
        return False
    return any(keyword in text for keyword in training_keywords)

def final_judgment(row):
    body = row.Body
    # must keep í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ìš°ì„  'ì‚´ë ¤ì•¼í•¨'ìœ¼ë¡œ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ ë³€ê²½)
    if row.recovery_candidate:
        return 'ì‚´ë ¤ì•¼í•¨'
    elif is_irrelevant_context(body) or is_training_news(body):
        return 'ê·¸ëŒ€ë¡œ ê±¸ëŸ¬ì•¼í•¨'
    else:
        return row.judgment


# ---------------------------
# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
# ---------------------------
collected_files = [f for f in os.listdir(article_folder_path)
                   if f.startswith("005930_20") and f.endswith(".csv")]

all_articles = []

for file in tqdm(collected_files, desc="ğŸ“‚ íŒŒì¼ ì½ëŠ” ì¤‘"):
    path = os.path.join(article_folder_path, file)
    df = pd.read_csv(path)
    if 'Body' in df.columns and not df.empty:
        df = df[['Title','Date','Link','Press','Body','Emotion','Comment_body','Comment_date','Comment_recomm','Comment_unrecomm','Num_comment','ID','version']].copy()
        df['source_file'] = file
        all_articles.append(df)
    else:
        print(f"âš ï¸ '{file}' íŒŒì¼ì— 'Body' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

if not all_articles:
    raise ValueError("ğŸš¨ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ê³¼ ì»¬ëŸ¼ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
all_articles_df = pd.concat(all_articles, ignore_index=True)

# ---------------------------
# TF-IDF ê³„ì‚°
# ---------------------------
vectorizer = TfidfVectorizer(max_features=1000, stop_words=[
    'ê²ƒ', 'ìˆ˜', 'ë“±', 'ì´í›„', 'ê´€ë ¨', 'ìœ„í•´', 'ëŒ€í•´', 'í†µí•´', 'ëŒ€í•œ',
    'ê¸°ì', 'ë³´ë„', 'ìˆë‹¤', 'ì´ë²ˆ', 'í–ˆë‹¤', 'í–ˆë‹¤ë©°'
])
tfidf_matrix = vectorizer.fit_transform(all_articles_df['Body'].fillna(''))

scores = tfidf_matrix.sum(axis=1).A1
threshold = pd.Series(scores).quantile(tfidf_top_percentile)

all_articles_df['tfidf_score'] = scores
all_articles_df['judgment'] = all_articles_df['tfidf_score'].apply(
    lambda x: 'ì‚´ë ¤ì•¼í•¨' if x >= threshold else 'ê·¸ëŒ€ë¡œ ê±¸ëŸ¬ì•¼í•¨'
)

# ---------------------------
# ì‹¤ì  í‚¤ì›Œë“œ ë³µêµ¬ íŒë‹¨ (tqdm ì ìš©)
# ---------------------------
print("ğŸ” ì‹¤ì /ìœ„ê¸° í‚¤ì›Œë“œ í™•ì¸ ì¤‘...")
all_articles_df['recovery_candidate'] = [
    is_must_keep(text) for text in tqdm(all_articles_df['Body'], desc="ğŸ’¡ í‚¤ì›Œë“œ ë³µêµ¬ íŒë‹¨")
]

# ---------------------------
# ìµœì¢… íŒë‹¨ (tqdm ì ìš©)
# ---------------------------
print("ğŸ§  ìµœì¢… í•„í„° íŒë‹¨ ì¤‘...")
all_articles_df['judgment'] = [
    final_judgment(row) for row in tqdm(all_articles_df.itertuples(), total=len(all_articles_df), desc="ğŸ“ ìµœì¢… íŒë‹¨ ì ìš©")
]

# ---------------------------
# ê²°ê³¼ ì €ì¥
# ---------------------------
output_file_keep = os.path.join(article_folder_path, "classified_articles_keep_extra.csv")
output_file_discard = os.path.join(article_folder_path, "classified_articles_discard_extra.csv")

all_articles_df[all_articles_df['judgment'] == 'ì‚´ë ¤ì•¼í•¨'].to_csv(output_file_keep, index=False, encoding='utf-8-sig')
all_articles_df[all_articles_df['judgment'] == 'ê·¸ëŒ€ë¡œ ê±¸ëŸ¬ì•¼í•¨'].to_csv(output_file_discard, index=False, encoding='utf-8-sig')

print(f"\nâœ… TF-IDF + í‚¤ì›Œë“œ í•„í„° ì ìš© ì™„ë£Œ:")
print(f" - ì‚´ë ¤ì•¼í•¨: {output_file_keep}")
print(f" - ê±¸ëŸ¬ì•¼í•¨: {output_file_discard}")